
This looks like a good place to start learning about RL coding:
https://github.com/matthiasplappert/keras-rl


steps:

git clone https://github.com/matthiasplappert/keras-rl
ln -s keras-rl krl

study:
https://github.com/matthiasplappert/keras-rl#how-do-i-install-it-and-how-do-i-get-started

pip install keras-rl

I dont have pip

I should try conda install keras-rl
that fails
Try          conda install keras-rl -c conda-forge
that fails

I should try to install keras:
conda install keras
that worked

I should try:
pip install keras-rl

It worked:
agent4@agent4:~/krl $ pip install keras-rl
Collecting keras-rl
  Downloading keras-rl-0.4.0.tar.gz
Requirement already satisfied: keras>=2.0.7 in /home/agent4/anaconda3/lib/python3.6/site-packages (from keras-rl)
Requirement already satisfied: numpy>=1.9.1 in /home/agent4/anaconda3/lib/python3.6/site-packages (from keras>=2.0.7->keras-rl)
Requirement already satisfied: scipy>=0.14 in /home/agent4/anaconda3/lib/python3.6/site-packages (from keras>=2.0.7->keras-rl)
Requirement already satisfied: six>=1.9.0 in /home/agent4/anaconda3/lib/python3.6/site-packages (from keras>=2.0.7->keras-rl)
Requirement already satisfied: pyyaml in /home/agent4/anaconda3/lib/python3.6/site-packages (from keras>=2.0.7->keras-rl)
Building wheels for collected packages: keras-rl
  Running setup.py bdist_wheel for keras-rl ... [?25ldone
[?25h  Stored in directory: /home/agent4/.cache/pip/wheels/63/67/ab/e485ebd7deb431c6d1470eee62fb1f8b553f4c4cd034b576c6
Successfully built keras-rl
Installing collected packages: keras-rl
Successfully installed keras-rl-0.4.0
agent4@agent4:~/krl $ 
agent4@agent4:~/krl $


2018-01-15

I should find something in krl I can run...

If you want to run the examples, you'll also have to install `gym` by OpenAI.
https://github.com/openai/gym#installation

You'll also need the `h5py` package

conda list|grep h5py

gym:
   -
You can perform a minimal install of gym with:

git clone https://github.com/openai/gym.git
cd gym
pip install -e .

I see good news:
cd
agent4@agent4:~ $ 
agent4@agent4:~ $ git clone https://github.com/openai/gym.git
Cloning into 'gym'...
remote: Counting objects: 6241, done.        
remote: Total 6241 (delta 0), reused 0 (delta 0), pack-reused 6241        
Receiving objects: 100% (6241/6241), 1.54 MiB | 0 bytes/s, done.
Resolving deltas: 100% (4210/4210), done.
Checking connectivity... done.
agent4@agent4:~ $ cd gym
agent4@agent4:~/gym $ pip install -e .
Obtaining file:///home/agent4/gym
Requirement already satisfied: numpy>=1.10.4 in /home/agent4/anaconda3/lib/python3.6/site-packages (from gym==0.9.3)
Requirement already satisfied: requests>=2.0 in /home/agent4/anaconda3/lib/python3.6/site-packages (from gym==0.9.3)
Requirement already satisfied: six in /home/agent4/anaconda3/lib/python3.6/site-packages (from gym==0.9.3)
Collecting pyglet>=1.2.0 (from gym==0.9.3)
  Downloading pyglet-1.3.0-py2.py3-none-any.whl (1.0MB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 1.1MB/s 
[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/agent4/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym==0.9.3)
Requirement already satisfied: idna<2.7,>=2.5 in /home/agent4/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym==0.9.3)
Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/agent4/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym==0.9.3)
Requirement already satisfied: certifi>=2017.4.17 in /home/agent4/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym==0.9.3)
Collecting future (from pyglet>=1.2.0->gym==0.9.3)
  Downloading future-0.16.0.tar.gz (824kB)
    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 87kB/s 
[?25hBuilding wheels for collected packages: future
  Running setup.py bdist_wheel for future ... [?25ldone
[?25h  Stored in directory: /home/agent4/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a
Successfully built future
Installing collected packages: future, pyglet, gym
  Running setup.py develop for gym
Successfully installed future-0.16.0 gym pyglet-1.3.0
agent4@agent4:~/gym $ conda list |grep gym
gym                       0.9.3                     <pip>
agent4@agent4:~/gym $ conda list|grep keras
keras                     2.1.2                    py36_0  
keras-rl                  0.4.0                     <pip>
agent4@agent4:~/gym $




Once you have installed everything, you can try out a simple example:
```bash
python examples/dqn_cartpole.py
```


agent4@agent4:~/krl $ ll examples/
total 48
drwxrwxr-x 2 agent4 agent4 4096 Jan 15 00:32 ./
drwxrwxr-x 8 agent4 agent4 4096 Jan 15 13:36 ../
-rw-rw-r-- 1 agent4 agent4 1826 Jan 15 00:32 cem_cartpole.py
-rw-rw-r-- 1 agent4 agent4 2798 Jan 15 00:32 ddpg_mujoco.py
-rw-rw-r-- 1 agent4 agent4 2575 Jan 15 00:32 ddpg_pendulum.py
-rw-rw-r-- 1 agent4 agent4 5382 Jan 15 00:32 dqn_atari.py
-rw-rw-r-- 1 agent4 agent4 1670 Jan 15 00:32 dqn_cartpole.py
-rw-rw-r-- 1 agent4 agent4 2049 Jan 15 00:32 duel_dqn_cartpole.py
-rw-rw-r-- 1 agent4 agent4 3257 Jan 15 00:32 naf_pendulum.py
-rw-rw-r-- 1 agent4 agent4 1452 Jan 15 00:32 sarsa_cartpole.py
-rw-rw-r-- 1 agent4 agent4 1553 Jan 15 00:32 visualize_log.py
agent4@agent4:~/krl $ 
agent4@agent4:~/krl $ 
agent4@agent4:~/krl $


agent4@agent4:~/krl $ cat examples/dqn_cartpole.py 
import numpy as np
import gym

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory


ENV_NAME = 'CartPole-v0'


# Get the environment and extract the number of actions.
env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n

# Next, we build a very simple model.
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
print(model.summary())

# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and
# even the metrics!
memory = SequentialMemory(limit=50000, window_length=1)
policy = BoltzmannQPolicy()
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
               target_model_update=1e-2, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])

# Okay, now it's time to learn something! We visualize the training here for show, but this
# slows down training quite a lot. You can always safely abort the training prematurely using
# Ctrl + C.
dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)

# After training is done, we save the final weights.
dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)

# Finally, evaluate our algorithm for 5 episodes.
dqn.test(env, nb_episodes=5, visualize=True)
agent4@agent4:~/krl $ 
agent4@agent4:~/krl $ 
agent4@agent4:~/krl $


agent4@agent4:~/krl $ mkdir dan
agent4@agent4:~/krl $ cp examples/dqn_cartpole.py dan/
agent4@agent4:~/krl $

vi dan/d*
python dan/d*


It works!

I should start work on class01

ooooooooooo

I should create a vbox:
  rl2016_0115.ova
  user0: a4
  
