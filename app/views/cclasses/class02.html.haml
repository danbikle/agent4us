%h1 Class02

%p This class is under construction.

%p
  The previous class focused on understanding some ideas behind
  %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py.

%p This class should help you get a better understanding of openai-gym software.

%p
  Q: How does
  %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
  connect to openai-gym?

%ul
  %li First, in the above script, I see this line of syntax:
  %li
    .syntax
      %pre
        %code.python ENV_NAME = 'CartPole-v0'
  %li Then I create an env-object:
  %li
    .syntax
      %pre
        %code.python env = gym.make(ENV_NAME)

  %li It seems probable that this env-object is used to connect the dqn-object to gym.
  
  %li Later env appears in the call to dqn.fit()

  %li
    .syntax
      %pre
        %code.python dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)

  
  %li Finally, env appears in the call to dqn.test()

  %li
    .syntax
      %pre
        %code.python dqn.test(env, nb_episodes=5, visualize=True)

  %li to gain understanding about gym, I should study the interaction between env and dqn.
  %li Before I do that I watch some of this video at time 29.5min:
  %li
    %a(href='https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1770' target='x')
      https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1770
  %li
    According to the above video, the agent in our case, dqn,
    %br/
    should see two types of information coming from the env: Observations and Rewards.
  %li How to see Observations and Rewards coming from env into dqn?
  %li I suspect I might see them if I step into dqn.fit() with pdb.
  %li I used a shell to do that:
  %li

    .syntax
      %pre
        =render 'class02a10'

  %li
    After using the above interactions as a guide, I quickly found that the "hot" syntax
    %br/
    in dqn.fit() starts at line 116 and ends at line 200

  %li
    Also I can see that the main way to get observation and reward out of env
    %br/
    is with this call which is on line 168:
  %li
    .syntax
      %pre
        %code.python observation, r, done, info = env.step(action)
  %li
    It is interesting that Matthias added what appears to be a mechanism for the agent
    %br/
    to alter observation and reward via a method called "processor".

  %li On line 170 I see processor in an if statement:
  %li
    .syntax
      %pre
        %code.python observation, r, done, info = self.processor.process_step(observation, r, done, info)

  %li I started a study of processor:
  %li
    .syntax
      %pre
        =render 'class02a12'

  %li According to the Silver video, the agent has a "state".
  %li Is that concept visible in any of the syntax which runs when I call dqn.fit() ?
  
%hr/
