%h1 Class02

%p This class is under construction.

%p
  The previous class focused on understanding some ideas behind
  %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py.

%p This class should help you get a better understanding of openai-gym software.

%p
  Q: How does
  %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
  connect to openai-gym?

%ul
  %li First, in the above script, I see this line of syntax:
  %li
    .syntax
      %pre
        %code.python ENV_NAME = 'CartPole-v0'
  %li Then I create an env-object:
  %li
    .syntax
      %pre
        %code.python env = gym.make(ENV_NAME)

  %li It seems probable that this env-object is used to connect the dqn-object to gym.
  
  %li Later env appears in the call to dqn.fit()

  %li
    .syntax
      %pre
        %code.python dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)

  
  %li Finally, env appears in the call to dqn.test()

  %li
    .syntax
      %pre
        %code.python dqn.test(env, nb_episodes=5, visualize=True)

  %li To gain understanding about gym, I should study the interaction between env and dqn.
  %li Before I do that, I watch some of this video at time 29.5min:
  %li
    %a(href='https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1770' target='x')
      https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1770
  %li
    According to the above video, the agent in our case, dqn,
    %br/
    should see two types of information coming from the env: Observations and Rewards.
  %li How to see Observations and Rewards coming from env into dqn?
  %li I suspect I might see them if I step into dqn.fit() with pdb.
  %li I used a shell to do that:
  %li

    .syntax
      %pre
        =render 'class02a10'

  %li
    After using the above interactions as a guide, I quickly found that the "hot" syntax
    %br/
    in dqn.fit() starts at line 116 and ends at line 200

  %li
    Also I can see that the main way to get observation and reward out of env
    %br/
    is with this call which is on line 168:
  %li
    .syntax
      %pre
        %code.python observation, r, done, info = env.step(action)
  %li
    It is interesting that Matthias added what appears to be a mechanism for the agent
    %br/
    to alter observation and reward via a method called "processor".

  %li On line 170 I see processor in an if statement:
  %li
    .syntax
      %pre
        %code.python observation, r, done, info = self.processor.process_step(observation, r, done, info)

  %li I started a study of processor:
  %li
    .syntax
      %pre
        =render 'class02a12'

  %li Next, to understand CartPole-env a little more, I wrote a simple script to call its step() method.
  
  %li
    .syntax
      %pre
        =render 'class02a14'

  %li I ran it and saw this:

  %li
    .syntax
      %pre
        =render 'class02a16'
  
  %li The above output appears to be quite simple.    
  %li I found that the above output helped me understand CartPole-env a little bit more.
  %li While studying CartPole-env above, I asked a question:
  %li Q: How can I create an environment (sm-env) which rewards me from stock-market decisions?
  %li A: I use a scenario to describe how sm-env might behave:
  %li - I pull $100 out of my bank account and buy $100 of AAPL which is trading at $100 / share
  %li - I wait
  %li - I sell 1 share of AAPL for $110
  %li - I put that $110 in my bank account
  %li I like the above scenario because bank-account-balance fits nicely to an RL-Goal.
  %li My goal is to maximize my bank-account-balance.
  %li Remember that RL-Goal is defined as cumulative sum of Rewards from an Environment.
  %li
    %a(href='https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1380' target='x')
      https://www.youtube.com/watch?v=2pWv7GOvuf0&t=1380
  
  %li
    %img(src='/class02/class02a18.png')
  %li

%hr/
