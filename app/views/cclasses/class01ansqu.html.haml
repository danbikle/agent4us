%h1
  Answer questions about some API calls in
  %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py

%ul
  %li
    Where do I find
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py?
  %li
    .syntax
      %pre
        =render 'class01ansqu10'
        
  %li
    Can you describe a use-case solved by
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py?
  %li
    %img(src='/class01/class01ansqu11.png')

  %li

    .syntax
      %pre
        =render 'class01ansqu12'

  %li
    The above idea can be visualized with a diagram I see on the
    %a(href='http://web.stanford.edu/class/cs234/index.html' target='s') Stanford-CS234
    website:
  %li
    %img(src='/class01/class01ansqu13.png')
  %li The output of the Agent is an Action which depends on a Policy function I should write.
  %li If the Policy and Actions work well, the World sends me a signal called: "Reward".
  %li Additionally the World sends me Observations which I feed into my Policy function.
  %li Note that the gym-API, discussed below, uses the synonym Environment instead of World.

  %li
    Can you describe the imports at the top of
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py?
  %li
    .syntax
      %pre
        =render 'class01ansqu14'

  %li
    After the imports, I see a small group of syntax:
    .syntax
      %pre
        =render 'class01ansqu16'
  %li What does that syntax do?
  %li 
    .syntax
      %pre
        =render 'class01ansqu18'
  %li
    %a(href='https://www.google.com/search?q=In+Reinforcement+Learning,+What+is+an+Episode' target='x')
      https://www.google.com/search?q=In+Reinforcement+Learning,+What+is+an+Episode?
  %li 
    .syntax
      %pre
        %code
          An example of an episode is one chess match.
          For the cart-pole demo an episode will end in a variety of ways, for example if the pole falls over.
          More info:
          %a(href='https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning' target='x')
            https://stats.stackexchange.com/questions/250943/what-is-the-difference-between-episode-and-epoch-in-deep-q-learning
          %a(href='https://www.quora.com/What-does-the-term-%E2%80%9Cepisode%E2%80%9D-mean-in-the-context-of-reinforcement-learning-RL' target='x')
            https://www.quora.com/What-does-the-term-%E2%80%9Cepisode%E2%80%9D-mean-in-the-context-of-reinforcement-learning-RL
  

  %li
    Returning my attention to:
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    I step into the next line of syntax with the pdb-debugger:
    
    .syntax
      %pre
        %code.python env.seed(123)
        
    .syntax
      %pre
        =render 'class01ansqu20'
        
    So, that call: "Sets the seed for this env's random number generator(s)."
  

  %li
    I used the debugger again to study the next line of syntax:
    
    .syntax
      %pre
        %code.python nb_actions = env.action_space.n
        
    .syntax
      %pre
        =render 'class01ansqu22'

    I see that call getting an integer from an attribute called action_space.
    %br/
    This attribute is a function call: Discrete(2).
    
    %ul
      %li
        %a(href='https://www.google.com/search?q=In+openai+gym,+what+is+Discrete()' target='x')
          https://www.google.com/search?q=In+openai+gym,+what+is+Discrete()?
      %li
  
        This URL gives a simple description:
      %li
        %a(href='https://gym.openai.com/docs/' target='x')
          https://gym.openai.com/docs/
  
      %li
        Every environment comes with first-class Space objects that describe
        %br/
        the valid actions and observations.
      %li

        The Discrete space allows a fixed range of non-negative numbers,
        %br/
        so in this case valid actions are either 0 or 1.
        %br/
        The Box space represents an n-dimensional box,
        %br/
        so valid observations will be an array of 4 numbers.
      %li

        Box and Discrete are the most common Spaces.
      %li
        For CartPole-v0 one of the actions applies force to the left,
        %br/
        and one of them applies force to the right.

    So, Matthias is setting up a Discrete space which allows the agent
    %br/
    to push the cart in the 0-direction or the 1-direction which might be left or right.
    %br/
    Q: Why is Matthias getting the number of actions from env.action_space.n?
    %br
    A: He needs this integer at two places later in the script:
    %ul
      %li He uses nb_actions to the define number of outputs of a Neural-Net.
      %li He uses nb_actions to help initialize DQNAgent() which constructs an agent for the cart.


  %li
    I used Google to study the next section in the script which is displayed below:

    .syntax
      %pre
        =render 'class01ansqu24'
  %li
    %a(href='https://www.google.com/search?q=In+Keras+What+is+Sequential' target='x')
      https://www.google.com/search?q=In+Keras+What+is+Sequential?
  
  %li
    Google returned excellent results from the above question.
    %br/
    I like this result the most:
    %br/
    %a(href='https://keras.io/getting-started/sequential-model-guide/' target='x')
      https://keras.io/getting-started/sequential-model-guide/
    %br/
    The idea of the API is simple.  
    %br/
    I create an empty model object and then add neural-net layers to it.
    %br/
    When I am done, I can visualize my model as something like this:
    %br/
    %img(src='/class01/class01ansqu26.jpg')
    %br/
    Matthias specified an output layer which has two outputs, as specified by nb_actions, instead of four.    
  %li Next, I studied this line of sytnax:
  
  %li
    .syntax
      %pre
        %code.python model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
        
  %li I studied that line with the debugger:
  
  %li
    .syntax
      %pre
        =render 'class01ansqu28'
    
  %li model.summary() is useful; it tells me the input layer has 4 neurons.
  %li We saw twice above, that gym will be sending observations which are an array of 4 numbers.
  %li Matthias ensures this match happens by getting the shape of each observation with this expression:
  %li
  
    .syntax
      %pre
        %code.python env.observation_space.shape
        
  %li The summary shows that this first layer is of type "Flatten".
  %li I asked google about that:
  %li
    %a(href='https://www.google.com/search?q=In+Keras+what+is+Flatten+Layer' target='x')
      https://www.google.com/search?q=In+Keras+what+is+Flatten+Layer?
  %li So, it transforms an input with any nested-structure into a 1-D structure.
  %li Why does Matthias constrain this layer with Flatten?
  %li I assume he wants to ensure that each observation matches nicely with the input layer.
  %li Remember that in RL, an observation is a piece of data flowing from environment into agent.
  %li
    Obviously, when he built
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    he knew that observations would be one-dimensional.
    
  %li Next, I studied this line of sytnax:
  
  %li
    .syntax
      %pre
        %code.python model.add(Dense(16))
        
  %li I asked google a simple question:
  %li
    %a(href='https://www.google.com/search?q=In+Keras+when+I+add+layer+what+is+Dense' target='x')
      https://www.google.com/search?q=In+Keras+when+I+add+layer+what+is+Dense?
  %li I liked these links returned by above search:
  %ul
    %li
      %a(href='https://keras.io/layers/core/#dense' target='x')
        https://keras.io/layers/core/#dense
    %li
      %a(href='https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer' target='x')
        https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer
  %li So, that line of syntax adds a layer after the input layer.  
  %li
    This second layer is densely connected to the input layer;
    %br/
    meaning that each of the 4 neurons in the first layer connects to all 16 neurons in the next layer.
  %li Q: How does Matthias define each connection?
  %li A: He uses this next line of syntax:
  
  %li
    .syntax
      %pre
        %code.python model.add(Activation('relu'))
   
  %li I asked google about that:
  %li
    %a(href='https://www.google.com/search?q=In+Keras+add+what+is+Activation+relu' target='x')
      https://www.google.com/search?q=In+Keras+add+what+is+Activation+relu?

  %li I liked these answers:
  %ul
    %li
      %a(href='https://keras.io/activations/' target='x')
        https://keras.io/activations/
    %li
      %a(href='https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02' target='x')
        https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02
  %li I visualize Activation('relu') as similar to my understanding of the signal out of a biological neuron.
  %li Much of the time the neuron issues no signal.
  %li I can make the neuron "fire" by sending it a positive electrical current.
  %li The intensity of the output is a linear response to the positive input.
  %li If I send it a negative electrical current, it does nothing.
  %li This is similar to the behavior of an electrical component called a "Diode".
  %li
    %a(href='https://www.google.com/search?q=In+electronics+what+is+a+diode' target='x')
      https://www.google.com/search?q=In+electronics+what+is+a+diode?
  %li
    I can visualize the behavior with this plot from
    %a(href='https://towardsdatascience.com/@shudima' target='c') Dima Shulga:
  %li
    %img(src='/class01/class01ansqu28.png')
  %li
    In a biological system this makes sense. Suppose I want to raise my arm.
    %br/
    I need to send it a signal, a positive signal.
    %br/
    If I want the arm to raise quickly, I send it more signal.
    %br/
    If I want the arm stationary, I send it no signal.
    %br/
    I cannot conceive of a situation when I want to send a negative signal to my arm.


  %li
    Returning my attention to:
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    I continue stepping through what I call the Keras-model-creation section:
    
  %li

    .syntax
      %pre
        =render 'class01ansqu24'
  %li
    After the script created the three hidden layers, I looked at a model.summary():
  %li

    .syntax
      %pre
        =render 'class01ansqu30'
        
  %li What I see above is what I expect: 
  %li A densely connected network with a 4-neuron input layer and 3 hidden layers each with 16 neurons. 

  %li
    I stepped through two more lines:

  %li
    .syntax
      %pre
        =render 'class01ansqu32'
  %li
    Notice the last activation is 'linear'.
  %li
    I am curious to know how linear signals from this final layer will eventually
    %br/
    get translated to 0 or 1 when the Agent needs to issue an action.


  %li
    Returning my attention to:
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    I study the next section:
    
  %li
  
    .syntax
      %pre
        =render 'class01ansqu34'
  %li The first three lines of syntax configure our Agent.
  %li The fourth line compiles it.

  %li I looked at the first line and asked Google a question:
  %li
    %a(href='https://www.google.com/search?q=In+keras-rl+what+is+SequentialMemory?' target='x')
      https://www.google.com/search?q=In+keras-rl+what+is+SequentialMemory?
  %li
    That led me to an insightful comment in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/memory.py' target='x') memory.py:
    
  %li
    .syntax
      %pre
        %code.python
          \# Do not use deque to implement the memory. This data structure may seem convenient but
          \# it is way too slow on random access. Instead, we use our own ring buffer implementation.  

  %li
    So, SequentialMemory is like
    %a(href='https://docs.python.org/3/library/collections.html#collections.deque' target='x') collections.deque
    , a popular class in the collections package, but faster.

  %li I looked at the second line and asked Google a question:
  %li
    %a(href='https://www.google.com/search?q=In+keras-rl+what+is+BoltzmannQPolicy()' target='x')
      https://www.google.com/search?q=In+keras-rl+what+is+BoltzmannQPolicy()?
  %li The first result was this link:  
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/policy.py' target='x')
      https://github.com/matthiasplappert/keras-rl/blob/master/rl/policy.py
  %li The syntax for that API call was short:
  %li

    .syntax
      %pre
        =render 'class01ansqu36'

  %li If you are new to Python you might have some questions about the above syntax:
  %li What does this syntax do: class BoltzmannQPolicy(Policy): ?
  %li
    %a(href='https://www.google.com/search?q=In+Python+why+would+I+pass+a+parameter+to+a+class+definition' target='x')
      https://www.google.com/search?q=In+Python+why+would+I+pass+a+parameter+to+a+class+definition?
  %li Answer:
  %li
    %a(href='https://docs.python.org/3.6/tutorial/classes.html#inheritance' target='x')
      https://docs.python.org/3.6/tutorial/classes.html#inheritance
  %li
    %a(href='https://www.google.com/search?q=In+Python+What+does+this+method+do+def+__init__()' target='x')
      https://www.google.com/search?q=In+Python+What+does+this+method+do+def+__init__()?
  %li Answer:
  %li
    %a(href='https://docs.python.org/3.6/tutorial/classes.html#class-objects' target='x')
      https://docs.python.org/3.6/tutorial/classes.html#class-objects
  %li
    %a(href='https://www.google.com/search?q=In+Python+what+does+super()+do' target='x')
      https://www.google.com/search?q=In+Python+what+does+super()+do?
  %li Answers:
  %li
    %a(href='https://docs.python.org/3.6/library/functions.html#super  ' target='x')
      https://docs.python.org/3.6/library/functions.html#super
      
  %li
    %a(href='https://rhettinger.wordpress.com/2011/05/26/super-considered-super/' target='x')
      https://rhettinger.wordpress.com/2011/05/26/super-considered-super/
  

  %li
    Returning my attention away from Python towards RL,
    %br/
    I see no comments in the class which is a disappointment
    %br/
    but I can see that select_action() selects an action from an ennumerable set.
    %br/
    And, the selection depends on a combination of q_values, randomness and probabilities.
  %li
    %a(href='https://www.google.com/search?q=In+Q-Learning+what+are+Q+values?' target='x')
      https://www.google.com/search?q=In+Q-Learning+what+are+Q+values?
  %li Answer: A Q-value quantifies "Quality" of a State-Action pair.
  %li In the context of the above method, a high Q-value for an action means I should see more reward soon if I pick that action.
  %li We can see from the syntax in select_action(), however, that more than q_values determines which action is selected.
  %li Here is some documentation which illuminates how Matthias mixes randomness and probabilities:
  %li
    %a(href='https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html    ' target='x')
      https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html
  %li The class below it has comments and they lead to an interesting link:
  %li
    .syntax
      %pre
        =render 'class01ansqu38'
  %li Again I asked Google a question:
  %li
    %a(href='https://www.google.com/search?q=In+keras-rl+what+is+BoltzmannQPolicy()' target='x')
      https://www.google.com/search?q=In+keras-rl+what+is+BoltzmannQPolicy()?
  

  %li The second link from that Google-search sent me to this page:
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/issues/18' target='x')
      https://github.com/matthiasplappert/keras-rl/issues/18
  %li
    The above discussion highlights that BoltzmannQPolicy policy implements the idea that
    %br/
    sometimes I should "explore" actions and sometimes I should "exploit" actions
    %br/
    known to be rewarding.
  %li If I only want to "exploit" actions I will pick the action with the highest Q-value.
  %li David Silver discusses this idea near the end of this video:
  %li
    %a(href='https://youtu.be/2pWv7GOvuf0?t=4849' target='x')
      https://youtu.be/2pWv7GOvuf0?t=4849
  %li
    %img(src='/class01/class01ansqu40.png')  
  %li
    %img(src='/class01/class01ansqu42.png')  
  %li
    %img(src='/class01/class01ansqu44.png')  


  %li
    Returning my attention to:
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    I study line 3 in what I call the Agent-config section:
  %li
    
    .syntax
      %pre
        =render 'class01ansqu46'
        
  %li I can see that DQNAgent() comes from this file:
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x')
      https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py
  %li Before studying that, I asked Google a general question:
  %li
    %a(href='https://www.google.com/search?q=In+Reinforcement+Learning+what+is+a+DQN+Agent' target='x')
      https://www.google.com/search?q=In+Reinforcement+Learning+what+is+a+DQN+Agent?
  %li Google returned many interesting links, I like the links listed below:
  %li
    %ul
      %li
        %a(href='https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf      ' target='x')
          https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf
      %li
        %a(href='http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html' target='x')
          http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
      %li
        %a(href='https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b' target='x')
          https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b
      %li
        %a(href='https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26' target='x')
          https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26
  
  %li
    Next, I started up Python and set a breakpoint at this line:
    
  %li
  
    .syntax
      %pre
        =render 'class01ansqu46'

  %li Then I stepped through many lines of synytax, like this:
    
  %li
  
    .syntax
      %pre
        =render 'class01ansqu48'
              
  %li I studied objects being fed to the constructor:  
  %li
  
    .syntax
      %pre
        =render 'class01ansqu50'
        
  %li Then I stepped into the call with the hope of studying my path through this file:
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x')
      https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py
      
  %li Like this:
  %li
  
    .syntax
      %pre
        =render 'class01ansqu52'

  %li After that session, I collected some questions:
  %li When I call DQNAgent() what is the parameter target_model_update used for?
  %li Answer:
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/issues/55  ' target='x')
      https://github.com/matthiasplappert/keras-rl/issues/55
  
  %li What is the parameter nb_steps_warmup used for?
  %li
    %a(href='https://github.com/matthiasplappert/keras-rl/issues/55#issuecomment-344257320' target='x')
      https://github.com/matthiasplappert/keras-rl/issues/55#issuecomment-344257320
  
  %li
    According to
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/core.py' target='x') core.py
    an agent requires I implement the following methods, how to do that?
  %li
    %ul
      %li forward       
      %li backward      
      %li compile       
      %li load_weights  
      %li save_weights  
      %li layers
      
  %li
    I see in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    that forward is implemented by class DQNAgent(AbstractDQNAgent):
  %li

    .syntax
      %pre
        =render 'class01ansqu54'
        
  %li When I study the above method I see that it does these steps:
  %li
    %ul
      %li get state
      %li compute_q_values(state)
      %li ask: am I training?
      %li return action
  %li
    %a(href='https://www.google.com/search?q=In+Deep+Q-Learning,+what+is+forward' target='x')
      https://www.google.com/search?q=In+Deep+Q-Learning,+what+is+forward?
  
  %li I found a discussion of forward here:
  %li
    %a(href='https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html' target='x')
      https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html

  %li
    I see in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    that backward is implemented by class DQNAgent():
  %li

    .syntax
      %pre
        =render 'class01ansqu56'

  %li I found a discussion of backward here:
  %li
    %a(href='https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html' target='x')
      https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html
  %li He says,
  %li "Use back-propagation and mini-batches stochastic gradient descent to update the network."
  %li What is the "network"?
  %li It is: "A CNN which transforms state into outputs. Outputs are Q-values of actions."
  %li The above URL has a nice image for this statement.
  %li The image shows state as being images from a game and output as Q-values.
  %li
    I think his concept of network corresponds to the keras-model which was defined in
    %br/
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py' target='x') dqn_cartpole.py
    and then fed to the DQNAgent() constructor.


  %li
    I see in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    that compile is implemented by class DQNAgent():
  %li

    .syntax
      %pre
        =render 'class01ansqu58'
  %li Q: What does compile() do in DQNAgent()?
  %li A: It creates target_model from model.
  %li A: Eventually it calls: trainable_model.compile() which is a Keras call.
  %li Much of compile() confuses me so I will ignore it for now.


  %li
    I see in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    that both load_weights() and save_weights() are implemented by class DQNAgent():
  %li

    .syntax
      %pre
        =render 'class01ansqu60'
  %li The above methods are simple wrappers of Keras methods:
  %li
    %a(href='https://keras.io/models/about-keras-models/' target='x')
      https://keras.io/models/about-keras-models/
  
  
  %li
    I see in
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    that layers is implemented by class DQNAgent():
  %li

    .syntax
      %pre
        =render 'class01ansqu62'
  %li It is a simple call which returns a list of layers in self.model.
  %li The @property makes the call a "property-getter" of the layers property for the DQNAgent class.
  %li
    If you do not understand this, just know that @property makes it easier for
    %br/
    you the developer to get certain data objects out of objects created from the DQNAgent class:
  %li
    %a(href='http://stackabuse.com/python-properties/' target='x')
      http://stackabuse.com/python-properties/
  %li
    Q:
    %a(href='https://www.google.com/search?q=In+keras-rl+AbstractDQNAgent+class+what+is+gamma+used+for' target='x')
      https://www.google.com/search?q=In+keras-rl+AbstractDQNAgent+class,+what+is+gamma+used+for?
  %li
    A:
    %a(href='https://stackoverflow.com/questions/17336069/setting-gamma-and-lambda-in-reinforcement-learning' target='x')
      https://stackoverflow.com/questions/17336069/setting-gamma-and-lambda-in-reinforcement-learning

  %li Q: In keras-rl DQNAgent class, what is train_interval used for?
  %li
    A: In
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    I found train_interval used in the backward() method.
  %li I see that if train_interval is small, backward() works harder.
  %li Q: Again, what is the purpose of backward()?
  %li
    A: We use backward() as part of the mechanism which trains
    %br/
    the Keras-model which predicts Q-values.
  %li Q: In keras-rl DQNAgent class, what is memory_interval used for?
  %li
    A: In
    %a(href='https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py' target='x') dqn.py
    I see that if memory_interval is small, backward() works harder.
  %li A: It works harder on this task: # Store most recent experience in memory.
  %li
    A: "experience" appears to be a tuple of
    %ul
      %li recent_observation
      %li recent_action
      %li reward
      %li terminal
      %li a boolean: self.training
  %li In keras-rl DQNAgent class, what is delta_range used for?
  %li In keras-rl DQNAgent class, what is delta_clip used for?
  %li In keras-rl, what is Processor?
  %li Why does DQN expect a model to have a single output?
  %li Why is test_policy = GreedyQPolicy()?
  %li What is GreedyQPolicy()?


  
%hr/    
%p Plain text format:

.syntax
  %pre
    =render 'class01ansqu1'

%a(href="/cclasses/class01#invbs") Lab Steps
%hr/
