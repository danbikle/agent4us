<code class='bash'>
agent4@agent4:~/us $ cd ~



agent4@agent4:~ $ cd keras-rl
agent4@agent4:~/keras-rl $ 



agent4@agent4:~/keras-rl $ python -m pdb examples/dqn_cartpole.py 
> /home/agent4/keras-rl/examples/dqn_cartpole.py(1)<module>()
-> import numpy as np



(Pdb) b 46
Breakpoint 1 at /home/agent4/keras-rl/examples/dqn_cartpole.py:46



(Pdb) c
Using TensorFlow backend.
[2018-01-23 17:05:22,075] Making new env: CartPole-v0
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 4)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 16)                80        
_________________________________________________________________
activation_1 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_2 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                272       
_________________________________________________________________
activation_3 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 2)                 34        
_________________________________________________________________
activation_4 (Activation)    (None, 2)                 0         
=================================================================
Total params: 658
Trainable params: 658
Non-trainable params: 0
_________________________________________________________________
None
> /home/agent4/keras-rl/examples/dqn_cartpole.py(46)<module>()
-> dqn.fit(env, nb_steps=50, visualize=False, verbose=0)



(Pdb) s
--Call--
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(44)fit()
-> def fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1,



(Pdb) b 109
Breakpoint 2 at /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py:109



(Pdb) c
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(109)fit()
-> self.step = 0



(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(110)fit()
-> observation = None
(Pdb)

> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(110)fit()
-> observation = None
(Pdb)



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(111)fit()
-> episode_reward = None
(Pdb)


> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(116)fit()
-> if observation is None:  # start of a new episode
(Pdb)



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(119)fit()
-> episode_reward = 0.
(Pdb)



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(123)fit()
-> observation = deepcopy(env.reset())
(Pdb)



(Pdb) observation
array([ 0.02078762, -0.01301236, -0.0209893 , -0.03935255])
(Pdb) 

Q: What do the above numbers mean?
A: Refer to this url:
   https://github.com/openai/gym/wiki/CartPole-v0#environment

Here is some information from that page:
<hr />
<p>Observation</p><p>Type: Box(4)</p><table border='1'><thead><tr><th>Num</th><th>Observation</th><th>Min</th><th>Max</th></tr></thead><tbody>
<tr>
<td>0</td>
<td>Cart Position</td>
<td>-2.4</td>
<td>2.4</td>
</tr>
<tr>
<td>1</td>
<td>Cart Velocity</td>
<td>-Inf</td>
<td>Inf</td>
</tr>
<tr>
<td>2</td>
<td>Pole Angle</td>
<td>~ -41.8°</td>
<td>~ 41.8°</td>
</tr>
<tr>
<td>3</td>
<td>Pole Velocity At Tip</td>
<td>-Inf</td>
<td>Inf</td></tr></tbody></table>
<hr />




> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(152)fit()
-> assert episode_reward is not None
(Pdb)

(Pdb) episode_reward
0.0
(Pdb)


> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(160)fit()
-> action = self.forward(observation)
(Pdb)



(Pdb) action
1
(Pdb)



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(163)fit()
-> reward = 0.
(Pdb)


Here is the section of the script I am now in:


                # Run a single step.
                callbacks.on_step_begin(episode_step)
                # This is were all of the work happens. We first perceive and compute the action
                # (forward step) and then use the reward to improve (backward step).
                action = self.forward(observation)
                if self.processor is not None:
                    action = self.processor.process_action(action)
                reward = 0.
                accumulated_info = {}
                done = False



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(166)fit()
-> for _ in range(action_repetition):
(Pdb)



(Pdb) action_repetition
1
(Pdb)



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(168)fit()
-> observation, r, done, info = env.step(action)
(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(169)fit()
-> observation = deepcopy(observation)
(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(170)fit()
-> if self.processor is not None:
(Pdb)

I typed r
CAREFUL r is a pdb command
startover

(Pdb) myr = r
(Pdb) myr
1.0
(Pdb)

So it appears that observation and r appear inside the agent via env.step(action)


> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(179)fit()
-> reward += r
(Pdb)

And I see above that fit() has a variable for cumulative reward.


> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(185)fit()
-> metrics = self.backward(reward, terminal=done)
(Pdb)


> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(186)fit()
-> episode_reward += reward
(Pdb)


next it does this:


                step_logs = {
                    'action': action,
                    'observation': observation,
                    'reward': reward,
                    'metrics': metrics,
                    'episode': episode,
                    'info': accumulated_info,
                }



> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(197)fit()
-> episode_step += 1
(Pdb)

(Pdb) episode_step
0
(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(198)fit()
-> self.step += 1
(Pdb) self.step
0
(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(200)fit()
-> if done:
(Pdb) done
False
(Pdb) n
> /home/agent4/anaconda3/lib/python3.6/site-packages/rl/core.py(115)fit()
-> while self.step &lt; nb_steps:
(Pdb) 



(Pdb) nb_steps
50
(Pdb)
</code>
