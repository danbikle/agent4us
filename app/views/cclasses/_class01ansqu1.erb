Where do I find dqn_cartpole.py?

I find it in the git repo I cloned to my home folder.

Remember that I had run this shell command:

git clone https://github.com/matthiasplappert/keras-rl

I find dqn_cartpole.py in the examples folder:

a4@a4:~$ ll keras-rl/examples/
total 48
drwxrwxr-x 2 a4 a4 4096 Jan 16 16:31 ./
drwxrwxr-x 8 a4 a4 4096 Jan 16 16:31 ../
-rw-rw-r-- 1 a4 a4 1826 Jan 16 16:31 cem_cartpole.py
-rw-rw-r-- 1 a4 a4 2798 Jan 16 16:31 ddpg_mujoco.py
-rw-rw-r-- 1 a4 a4 2575 Jan 16 16:31 ddpg_pendulum.py
-rw-rw-r-- 1 a4 a4 5382 Jan 16 16:31 dqn_atari.py
-rw-rw-r-- 1 a4 a4 1670 Jan 16 16:31 dqn_cartpole.py
-rw-rw-r-- 1 a4 a4 2049 Jan 16 16:31 duel_dqn_cartpole.py
-rw-rw-r-- 1 a4 a4 3257 Jan 16 16:31 naf_pendulum.py
-rw-rw-r-- 1 a4 a4 1452 Jan 16 16:31 sarsa_cartpole.py
-rw-rw-r-- 1 a4 a4 1553 Jan 16 16:31 visualize_log.py
a4@a4:~$ 
a4@a4:~$ 
a4@a4:~$ 

Also I find it on github.com:

https://github.com/matthiasplappert/keras-rl/blob/master/examples/dqn_cartpole.py



What is the main idea behind dqn_cartpole.py?

I see several ideas behind the demo:

- I use software to prevent the pole from falling over.
- I see an example of interacting with some APIs:
  - NumPy
  - gym
  - Keras
  - keras-rl
  - build neural-net layers in a Sequential Keras model
  - Link that model with keras-rl agent via dqn = DQNAgent(model=model,...)
  - Train the agent and link it with Environment via: dqn.fit(env, ...)
  - Save the training as a collection of weights stored via h5py-API into a h5f file
  
- In the script I see some RL related nouns, verbs, adjectives:
  - Agent
  - Environment
  - Optimizer
  - Adam
  - Action
  - DQNAgent
  - BoltzmannQPolicy
  - SequentialMemory
  - Training
  - Compile
  - Fit
  - Test

How to compare that main idea to a demonstration of using your brain to control a pole?

I would find a broom. Then, use my open palm (and brain) to keep the broom vertical.

What does the acronym DQN mean?

DQN is an acronym for Deep Q Learning:

https://www.google.com/search?q=Deep+Q+Learning
https://www.google.com/search?tbm=isch&q=Deep+Q+Learning
https://www.google.com/search?tbm=vid&q=Deep+Q+Learning
https://deepmind.com/research/dqn/


How does dqn_cartpole.py make use of the gym-package?

The gym-package is used to make an object called env:

This first usage is this API call:
# Get the environment and extract the number of actions.
env = gym.make(ENV_NAME)

Then these:

env.seed(123)
nb_actions = env.action_space.n

Then when Matthias builds the Keras sequential model, he needs to get some shape information from gym-env:

model.add(Flatten(input_shape=(1,) + env.observation_space.shape))

Then gym-env is used to help make the agent and train it:

# Okay, now it's time to learn something! We visualize the training here for show, but this
# slows down training quite a lot. You can always safely abort the training prematurely using
# Ctrl + C.
dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)

Then the name of this gym-env is used to save the training weights:

# After training is done, we save the final weights.
dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)


And, when I test, gym-env appears again:

# Finally, evaluate our algorithm for 5 episodes.
dqn.test(env, nb_episodes=5, visualize=True)



How does dqn_cartpole.py make use of the keras-rl package?

I think that I access keras-rl first with these imports:

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

I verify this with help of github:

https://github.com/matthiasplappert/keras-rl/blob/master/rl/agents/dqn.py

Although the git repo is called keras-rl, the name of the package is: "rl".

So, DQNAgent, BoltzmannQPolicy, SequentialMemory appear in the script while Matthias builds the agent.

I see that DQNAgent is a constructor.

BoltzmannQPolicy and SequentialMemory are called to create objects which are later fed to the constructor.


How does dqn_cartpole.py make use of the Keras package?

It uses the Keras package to create a neural network.

The syntax looks like this:

# Next, we build a very simple model.
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(16))
model.add(Activation('relu'))
model.add(Dense(nb_actions))
model.add(Activation('linear'))
print(model.summary())

I see similar syntax when I use Keras to build traditional Deep-Learning scripts.


Do you see anything in the syntax or behavior of dqn_cartpole.py which you can relate to David Silver's first video:
https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT
?

How about these videos:
https://www.youtube.com/watch?v=fIKkhoI1kF4
https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3

